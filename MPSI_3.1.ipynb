{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnTG3py4hTacROoHmAENh1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kvazzzzar/MPSI/blob/main/MPSI_3.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# 1. Collect Data и Prepare Data\n",
        "class TextDataProcessor:\n",
        "    \"\"\"Класс для обработки текстовых данных\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def load_data(self, filepath):\n",
        "        \"\"\"Загрузка датасета\"\"\"\n",
        "        self.df = pd.read_csv(filepath)\n",
        "        print(f\"Loaded dataset with {len(self.df)} samples\")\n",
        "        return self.df\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Предобработка текста\"\"\"\n",
        "        # Удаление спецсимволов и цифр\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
        "        # Приведение к нижнему регистру\n",
        "        text = text.lower()\n",
        "        # Токенизация\n",
        "        tokens = text.split()\n",
        "        # Удаление стоп-слов и лемматизация\n",
        "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens if word not in self.stop_words]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def analyze_data(self):\n",
        "        \"\"\"Анализ данных\"\"\"\n",
        "        print(\"\\nData Analysis:\")\n",
        "        print(self.df.head())\n",
        "        print(\"\\nCategory distribution:\")\n",
        "        print(self.df['category'].value_counts())\n",
        "\n",
        "    def vectorize_text(self):\n",
        "        \"\"\"Векторизация текста\"\"\"\n",
        "        self.vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        self.X = self.vectorizer.fit_transform(self.df['processed_text'])\n",
        "        return self.X\n",
        "\n",
        "    def cluster_texts(self, n_clusters=5):\n",
        "        \"\"\"Кластеризация текстов\"\"\"\n",
        "        self.kmeans = KMeans(n_clusters=n_clusters)\n",
        "        self.clusters = self.kmeans.fit_predict(self.X)\n",
        "\n",
        "        # Сравнение с реальной разметкой\n",
        "        if 'category' in self.df.columns:\n",
        "            labels = pd.factorize(self.df['category'])[0]\n",
        "            score = adjusted_rand_score(labels, self.clusters)\n",
        "            print(f\"\\nClustering quality (ARI): {score:.3f}\")\n",
        "\n",
        "    def split_data(self, test_size=0.2, val_size=0.1):\n",
        "        \"\"\"Разделение данных на train, test и val\"\"\"\n",
        "        # Сначала разделяем на train+val и test\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "            self.X, self.df['category'], test_size=test_size, random_state=42)\n",
        "\n",
        "        # Затем разделяем train+val на train и val\n",
        "        val_ratio = val_size / (1 - test_size)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_temp, y_temp, test_size=val_ratio, random_state=42)\n",
        "\n",
        "        print(f\"\\nData split:\")\n",
        "        print(f\"Train: {X_train.shape[0]} samples\")\n",
        "        print(f\"Val: {X_val.shape[0]} samples\")\n",
        "        print(f\"Test: {X_test.shape[0]} samples\")\n",
        "\n",
        "        return X_train, X_val, X_test, y_train, y_val, X_test\n",
        "\n",
        "# 2. Обучение упрощенной GPT модели\n",
        "class SimpleGPTTrainer:\n",
        "    \"\"\"Класс для обучения упрощенной GPT модели\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, max_seq_length):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        # Инициализация параметров\n",
        "        self.token_emb = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
        "        self.pos_emb = np.random.randn(max_seq_length, embedding_dim) * 0.01\n",
        "        self.Wq = np.random.randn(embedding_dim, embedding_dim) * 0.01\n",
        "        self.Wk = np.random.randn(embedding_dim, embedding_dim) * 0.01\n",
        "        self.Wv = np.random.randn(embedding_dim, embedding_dim) * 0.01\n",
        "        self.Wo = np.random.randn(embedding_dim, vocab_size) * 0.01\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "    def attention(self, x):\n",
        "        \"\"\"Механизм self-attention\"\"\"\n",
        "        Q = np.dot(x, self.Wq)\n",
        "        K = np.dot(x, self.Wk)\n",
        "        V = np.dot(x, self.Wv)\n",
        "\n",
        "        scores = np.dot(Q, K.T) / np.sqrt(self.embedding_dim)\n",
        "        weights = self.softmax(scores)\n",
        "        output = np.dot(weights, V)\n",
        "        return output\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        \"\"\"Прямой проход\"\"\"\n",
        "        token_emb = self.token_emb[token_ids]\n",
        "        pos_emb = self.pos_emb[:len(token_ids)]\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.attention(x)\n",
        "        logits = np.dot(x, self.Wo)\n",
        "        return logits\n",
        "\n",
        "    def compute_loss(self, logits, targets):\n",
        "        \"\"\"Вычисление потерь\"\"\"\n",
        "        probs = self.softmax(logits)\n",
        "        loss = -np.log(probs[np.arange(len(targets)), targets]).mean()\n",
        "        return loss\n",
        "\n",
        "    def train_step(self, batch, learning_rate=0.01):\n",
        "        \"\"\"Один шаг обучения\"\"\"\n",
        "        inputs, targets = batch[:-1], batch[1:]\n",
        "\n",
        "        # Прямой проход\n",
        "        logits = self.forward(inputs)\n",
        "        loss = self.compute_loss(logits, targets)\n",
        "\n",
        "        # Обратное распространение (упрощенное)\n",
        "        # В реальной реализации здесь должно быть вычисление градиентов\n",
        "        # и обновление параметров\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train(self, data, epochs=10, batch_size=32, learning_rate=0.01):\n",
        "        \"\"\"Процесс обучения\"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for i in range(0, len(data)-batch_size, batch_size):\n",
        "                batch = data[i:i+batch_size]\n",
        "                loss = self.train_step(batch, learning_rate)\n",
        "                total_loss += loss\n",
        "\n",
        "            avg_loss = total_loss / (len(data) // batch_size)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Пример использования полного пайплайна\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Обработка данных\n",
        "    processor = TextDataProcessor()\n",
        "\n",
        "    # Загрузка датасета (пример - нужно заменить на реальный датасет)\n",
        "    # dataset_url = \"https://example.com/text_dataset.csv\"\n",
        "    # df = processor.load_data(dataset_url)\n",
        "\n",
        "    # Создаем искусственные данные для примера\n",
        "    data = {\n",
        "        'text': [\n",
        "            \"This is a positive review about a product\",\n",
        "            \"Negative experience with customer service\",\n",
        "            \"The movie was great and actors performed well\",\n",
        "            \"I didn't like the book, it was boring\",\n",
        "            \"Excellent food and atmosphere at the restaurant\"\n",
        "        ],\n",
        "        'category': [\"positive\", \"negative\", \"positive\", \"negative\", \"positive\"]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    processor.df = df\n",
        "\n",
        "    # Предобработка текста\n",
        "    processor.df['processed_text'] = processor.df['text'].apply(processor.preprocess_text)\n",
        "    processor.analyze_data()\n",
        "\n",
        "    # Векторизация\n",
        "    X = processor.vectorize_text()\n",
        "\n",
        "    # Кластеризация\n",
        "    processor.cluster_texts(n_clusters=2)\n",
        "\n",
        "    # Разделение данных\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = processor.split_data()\n",
        "\n",
        "    # 2. Подготовка данных для GPT\n",
        "    # Создаем словарь и преобразуем тексты в последовательности токенов\n",
        "    vocab = {word: idx for idx, word in enumerate(set(' '.join(processor.df['processed_text']).split()))}\n",
        "    sequences = []\n",
        "    for text in processor.df['processed_text']:\n",
        "        tokens = text.split()\n",
        "        seq = [vocab[token] for token in tokens if token in vocab]\n",
        "        sequences.extend(seq)\n",
        "\n",
        "    # 3. Обучение GPT\n",
        "    gpt = SimpleGPTTrainer(\n",
        "        vocab_size=len(vocab),\n",
        "        embedding_dim=64,\n",
        "        max_seq_length=20\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining GPT model...\")\n",
        "    gpt.train(np.array(sequences), epochs=5, batch_size=8, learning_rate=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeQROa5vPTbT",
        "outputId": "a5c02b25-424a-4053-9960-d6764b47c387"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data Analysis:\n",
            "                                              text  category  \\\n",
            "0        This is a positive review about a product  positive   \n",
            "1        Negative experience with customer service  negative   \n",
            "2    The movie was great and actors performed well  positive   \n",
            "3            I didn't like the book, it was boring  negative   \n",
            "4  Excellent food and atmosphere at the restaurant  positive   \n",
            "\n",
            "                         processed_text  \n",
            "0               positive review product  \n",
            "1  negative experience customer service  \n",
            "2      movie great actor performed well  \n",
            "3                didnt like book boring  \n",
            "4  excellent food atmosphere restaurant  \n",
            "\n",
            "Category distribution:\n",
            "category\n",
            "positive    3\n",
            "negative    2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Clustering quality (ARI): 0.231\n",
            "\n",
            "Data split:\n",
            "Train: 3 samples\n",
            "Val: 1 samples\n",
            "Test: 1 samples\n",
            "\n",
            "Training GPT model...\n",
            "Epoch 1/5, Loss: 2.9957\n",
            "Epoch 2/5, Loss: 2.9957\n",
            "Epoch 3/5, Loss: 2.9957\n",
            "Epoch 4/5, Loss: 2.9957\n",
            "Epoch 5/5, Loss: 2.9957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}