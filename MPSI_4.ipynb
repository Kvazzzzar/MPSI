{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRNw0GhIdQbGREixh8zWYk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kvazzzzar/MPSI/blob/main/MPSI_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# Скачиваем необходимые ресурсы NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class TextDataProcessor:\n",
        "    \"\"\"\n",
        "    Класс для обработки текстовых данных от сбора до разделения на выборки\n",
        "    Включает:\n",
        "    - Загрузку датасета\n",
        "    - Предобработку текста\n",
        "    - Векторизацию\n",
        "    - Кластеризацию\n",
        "    - Разделение на train/test/val\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def load_dataset(self, path):\n",
        "        \"\"\"Загрузка датасета\"\"\"\n",
        "        self.df = pd.read_csv(path)\n",
        "        print(f\"Загружено {len(self.df)} записей\")\n",
        "        print(\"Пример данных:\")\n",
        "        print(self.df.head())\n",
        "        return self.df\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Предобработка текста: токенизация, лемматизация, очистка\"\"\"\n",
        "        # Приведение к нижнему регистру\n",
        "        text = text.lower()\n",
        "\n",
        "        # Удаление пунктуации\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        # Токенизация\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Удаление стоп-слов и лемматизация\n",
        "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens\n",
        "                 if token not in self.stop_words and token.isalpha()]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def analyze_data(self):\n",
        "        \"\"\"Анализ данных\"\"\"\n",
        "        print(\"\\nАнализ данных:\")\n",
        "        print(f\"Количество категорий: {len(self.df['category'].unique())}\")\n",
        "        print(\"Распределение категорий:\")\n",
        "        print(self.df['category'].value_counts())\n",
        "\n",
        "        # Длина текстов\n",
        "        self.df['text_length'] = self.df['text'].apply(lambda x: len(x.split()))\n",
        "        print(\"\\nСтатистика длины текстов:\")\n",
        "        print(self.df['text_length'].describe())\n",
        "\n",
        "    def vectorize_text(self):\n",
        "        \"\"\"Векторизация текста с помощью TF-IDF\"\"\"\n",
        "        self.vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        self.X = self.vectorizer.fit_transform(self.df['processed_text'])\n",
        "        print(f\"\\nРазмерность матрицы признаков: {self.X.shape}\")\n",
        "\n",
        "    def cluster_texts(self):\n",
        "        \"\"\"Кластеризация текстов и сравнение с реальными категориями\"\"\"\n",
        "        n_clusters = len(self.df['category'].unique())\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        clusters = kmeans.fit_predict(self.X)\n",
        "\n",
        "        # Сравнение с реальной разметкой\n",
        "        ari_score = adjusted_rand_score(self.df['category'], clusters)\n",
        "        print(f\"\\nAdjusted Rand Index (сравнение с реальными категориями): {ari_score:.2f}\")\n",
        "\n",
        "        # Добавляем кластеры в датасет для анализа\n",
        "        self.df['cluster'] = clusters\n",
        "\n",
        "    def split_data(self):\n",
        "        \"\"\"Разделение данных на train/test/val\"\"\"\n",
        "        # Сначала разделяем на train+val и test\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "            self.X, self.df['category'], test_size=0.15, random_state=42, stratify=self.df['category'])\n",
        "\n",
        "        # Затем разделяем train+val на train и val\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_temp, y_temp, test_size=0.15/0.85, random_state=42, stratify=y_temp)\n",
        "\n",
        "        print(\"\\nРазмеры выборок:\")\n",
        "        print(f\"Train: {X_train.shape[0]} samples\")\n",
        "        print(f\"Val: {X_val.shape[0]} samples\")\n",
        "        print(f\"Test: {X_test.shape[0]} samples\")\n",
        "\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "    def process_pipeline(self, dataset_path):\n",
        "        \"\"\"Полный пайплайн обработки данных\"\"\"\n",
        "        # 1. Collect data\n",
        "        self.load_dataset(dataset_path)\n",
        "\n",
        "        # 2. Prepare data\n",
        "        print(\"\\nПредобработка текстов...\")\n",
        "        self.df['processed_text'] = self.df['text'].apply(self.preprocess_text)\n",
        "\n",
        "        # 3. Analyze data\n",
        "        self.analyze_data()\n",
        "\n",
        "        # 4. Vectorize data\n",
        "        self.vectorize_text()\n",
        "\n",
        "        # 5. Cluster data\n",
        "        self.cluster_texts()\n",
        "\n",
        "        # 6. Split data\n",
        "        return self.split_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YipPPDGC-ity",
        "outputId": "51b4c585-6cf0-4912-b226-2145f858fa0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Скачать файл (пример для 20news-bydate.tar.gz)\n",
        "url = \"http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\"\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "with open(\"20news-bydate.tar.gz\", \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Распаковать\n",
        "with tarfile.open(\"20news-bydate.tar.gz\", \"r:gz\") as tar:\n",
        "    tar.extractall()\n",
        "\n",
        "# Проверить структуру\n",
        "!ls 20news-bydate-train  # Учебные данные\n",
        "!ls 20news-bydate-test   # Тестовые данные"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAcGxo6ELksk",
        "outputId": "e8db24cc-1ebc-4918-e8b4-886f75a4250d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alt.atheism\t\t  comp.windows.x      rec.sport.hockey\tsoc.religion.christian\n",
            "comp.graphics\t\t  misc.forsale\t      sci.crypt\t\ttalk.politics.guns\n",
            "comp.os.ms-windows.misc   rec.autos\t      sci.electronics\ttalk.politics.mideast\n",
            "comp.sys.ibm.pc.hardware  rec.motorcycles     sci.med\t\ttalk.politics.misc\n",
            "comp.sys.mac.hardware\t  rec.sport.baseball  sci.space\t\ttalk.religion.misc\n",
            "alt.atheism\t\t  comp.windows.x      rec.sport.hockey\tsoc.religion.christian\n",
            "comp.graphics\t\t  misc.forsale\t      sci.crypt\t\ttalk.politics.guns\n",
            "comp.os.ms-windows.misc   rec.autos\t      sci.electronics\ttalk.politics.mideast\n",
            "comp.sys.ibm.pc.hardware  rec.motorcycles     sci.med\t\ttalk.politics.misc\n",
            "comp.sys.mac.hardware\t  rec.sport.baseball  sci.space\t\ttalk.religion.misc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def load_20newsgroups_to_df(folder_path):\n",
        "    \"\"\"\n",
        "    Загружает данные 20 Newsgroups из структуры папок в DataFrame\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for category in Path(folder_path).iterdir():\n",
        "        if category.is_dir():\n",
        "            for file in category.iterdir():\n",
        "                with open(file, 'r', encoding='latin1') as f:\n",
        "                    text = f.read()\n",
        "                data.append({\n",
        "                    'text': text,\n",
        "                    'category': category.name,\n",
        "                    'source': 'train' if 'train' in str(folder_path) else 'test'\n",
        "                })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Загрузка train и test данных\n",
        "print(\"Загрузка train данных...\")\n",
        "df_train = load_20newsgroups_to_df(\"20news-bydate-train\")\n",
        "print(f\"Загружено {len(df_train)} train записей\")\n",
        "\n",
        "print(\"\\nЗагрузка test данных...\")\n",
        "df_test = load_20newsgroups_to_df(\"20news-bydate-test\")\n",
        "print(f\"Загружено {len(df_test)} test записей\")\n",
        "\n",
        "# Объединение данных\n",
        "print(\"\\nОбъединение данных...\")\n",
        "full_df = pd.concat([df_train, df_test], ignore_index=True)\n",
        "print(f\"Всего записей: {len(full_df)}\")\n",
        "\n",
        "# Анализ данных\n",
        "print(\"\\nРаспределение по категориям:\")\n",
        "print(full_df['category'].value_counts())\n",
        "\n",
        "print(\"\\nРаспределение по источникам:\")\n",
        "print(full_df['source'].value_counts())\n",
        "\n",
        "# Сохранение в CSV\n",
        "output_path = \"20news-full.csv\"\n",
        "full_df.to_csv(output_path, index=False)\n",
        "print(f\"\\nДанные сохранены в {output_path}\")\n",
        "\n",
        "# Пример первых записей\n",
        "print(\"\\nПервые 3 записи:\")\n",
        "print(full_df.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FXyA7HoLq-3",
        "outputId": "a6da5497-3ce9-424c-8862-b0323e953111"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка train данных...\n",
            "Загружено 11314 train записей\n",
            "\n",
            "Загрузка test данных...\n",
            "Загружено 7532 test записей\n",
            "\n",
            "Объединение данных...\n",
            "Всего записей: 18846\n",
            "\n",
            "Распределение по категориям:\n",
            "category\n",
            "rec.sport.hockey            999\n",
            "soc.religion.christian      997\n",
            "rec.motorcycles             996\n",
            "rec.sport.baseball          994\n",
            "sci.crypt                   991\n",
            "sci.med                     990\n",
            "rec.autos                   990\n",
            "comp.windows.x              988\n",
            "sci.space                   987\n",
            "comp.os.ms-windows.misc     985\n",
            "sci.electronics             984\n",
            "comp.sys.ibm.pc.hardware    982\n",
            "misc.forsale                975\n",
            "comp.graphics               973\n",
            "comp.sys.mac.hardware       963\n",
            "talk.politics.mideast       940\n",
            "talk.politics.guns          910\n",
            "alt.atheism                 799\n",
            "talk.politics.misc          775\n",
            "talk.religion.misc          628\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Распределение по источникам:\n",
            "source\n",
            "train    11314\n",
            "test      7532\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Данные сохранены в 20news-full.csv\n",
            "\n",
            "Первые 3 записи:\n",
            "                                                text   category source\n",
            "0  From: pef1@quads.uchicago.edu (it's enrico pal...  sci.space  train\n",
            "1  From: jbh55289@uxa.cso.uiuc.edu (Josh Hopkins)...  sci.space  train\n",
            "2  From: mrw9e@fulton.seas.Virginia.EDU (Michael ...  sci.space  train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataProcessor20News(TextDataProcessor):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def load_from_folder(self, folder_path):\n",
        "        \"\"\"Альтернативный метод загрузки данных из папки 20 Newsgroups\"\"\"\n",
        "        self.df = load_20newsgroups_to_df(folder_path)\n",
        "        print(f\"Загружено {len(self.df)} записей\")\n",
        "        print(\"Пример данных:\")\n",
        "        print(self.df.head())\n",
        "        return self.df"
      ],
      "metadata": {
        "id": "39x5qQmUMOU8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# 1. Инициализируем процессор\n",
        "processor = TextDataProcessor20News()\n",
        "\n",
        "# 2. Загружаем данные (используем объединенный CSV)\n",
        "processor.load_dataset(\"20news-full.csv\")  # Используем файл, который мы создали ранее\n",
        "\n",
        "# 3. Запускаем полный пайплайн обработки\n",
        "try:\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = processor.process_pipeline(\"20news-full.csv\")\n",
        "    print(\"\\nДанные успешно обработаны и разделены!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nПроизошла ошибка: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfHVm8U4MQDZ",
        "outputId": "7faff130-32bc-4359-fb26-9362d9c02483"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загружено 18846 записей\n",
            "Пример данных:\n",
            "                                                text   category source\n",
            "0  From: pef1@quads.uchicago.edu (it's enrico pal...  sci.space  train\n",
            "1  From: jbh55289@uxa.cso.uiuc.edu (Josh Hopkins)...  sci.space  train\n",
            "2  From: mrw9e@fulton.seas.Virginia.EDU (Michael ...  sci.space  train\n",
            "3  From: dietz@cs.rochester.edu (Paul Dietz)\\nSub...  sci.space  train\n",
            "4  From: aws@iti.org (Allen W. Sherzer)\\nSubject:...  sci.space  train\n",
            "Загружено 18846 записей\n",
            "Пример данных:\n",
            "                                                text   category source\n",
            "0  From: pef1@quads.uchicago.edu (it's enrico pal...  sci.space  train\n",
            "1  From: jbh55289@uxa.cso.uiuc.edu (Josh Hopkins)...  sci.space  train\n",
            "2  From: mrw9e@fulton.seas.Virginia.EDU (Michael ...  sci.space  train\n",
            "3  From: dietz@cs.rochester.edu (Paul Dietz)\\nSub...  sci.space  train\n",
            "4  From: aws@iti.org (Allen W. Sherzer)\\nSubject:...  sci.space  train\n",
            "\n",
            "Предобработка текстов...\n",
            "\n",
            "Анализ данных:\n",
            "Количество категорий: 20\n",
            "Распределение категорий:\n",
            "category\n",
            "rec.sport.hockey            999\n",
            "soc.religion.christian      997\n",
            "rec.motorcycles             996\n",
            "rec.sport.baseball          994\n",
            "sci.crypt                   991\n",
            "sci.med                     990\n",
            "rec.autos                   990\n",
            "comp.windows.x              988\n",
            "sci.space                   987\n",
            "comp.os.ms-windows.misc     985\n",
            "sci.electronics             984\n",
            "comp.sys.ibm.pc.hardware    982\n",
            "misc.forsale                975\n",
            "comp.graphics               973\n",
            "comp.sys.mac.hardware       963\n",
            "talk.politics.mideast       940\n",
            "talk.politics.guns          910\n",
            "alt.atheism                 799\n",
            "talk.politics.misc          775\n",
            "talk.religion.misc          628\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Статистика длины текстов:\n",
            "count    18846.000000\n",
            "mean       283.656001\n",
            "std        520.343541\n",
            "min         12.000000\n",
            "25%        107.000000\n",
            "50%        175.000000\n",
            "75%        292.000000\n",
            "max      11821.000000\n",
            "Name: text_length, dtype: float64\n",
            "\n",
            "Размерность матрицы признаков: (18846, 5000)\n",
            "\n",
            "Adjusted Rand Index (сравнение с реальными категориями): 0.13\n",
            "\n",
            "Размеры выборок:\n",
            "Train: 13192 samples\n",
            "Val: 2827 samples\n",
            "Test: 2827 samples\n",
            "\n",
            "Данные успешно обработаны и разделены!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Источники:\n",
        "\n",
        "http://qwone.com/~jason/20Newsgroups/ - 20news-bydate.tar.gz - 20 Newsgroups sorted by date; duplicates and some headers removed (18846 documents)"
      ],
      "metadata": {
        "id": "juDoYoOTLPv-"
      }
    }
  ]
}