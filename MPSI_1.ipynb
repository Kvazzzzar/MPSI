{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD1a2E6Ivl4v7HcX4GsEUn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kvazzzzar/MPSI/blob/main/MPSI_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2\n",
        "!pip install natasha\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w697RleKxXso",
        "outputId": "a9b05b79-2074-4546-fa82-f0cba7da908e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.11/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: natasha in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.11/dist-packages (from natasha) (0.9.1)\n",
            "Requirement already satisfied: razdel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from natasha) (0.5.0)\n",
            "Requirement already satisfied: navec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from natasha) (0.10.0)\n",
            "Requirement already satisfied: slovnet>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from natasha) (0.6.0)\n",
            "Requirement already satisfied: yargy>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from natasha) (0.16.0)\n",
            "Requirement already satisfied: ipymarkup>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from natasha) (0.9.0)\n",
            "Requirement already satisfied: intervaltree>=3 in /usr/local/lib/python3.11/dist-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from navec>=0.9.0->natasha) (1.26.4)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.11/dist-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.11/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенерезация по буквам\n",
        "токенайзер самому написать"
      ],
      "metadata": {
        "id": "_DxYyYcW3K3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pymorphy2\n",
        "import re\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Стоп-слова\n",
        "stop_words = {\"и\", \"в\", \"во\", \"не\", \"что\", \"он\", \"на\", \"я\", \"с\", \"со\", \"как\", \"а\", \"то\"}\n",
        "\n",
        "stemmer_ru = SnowballStemmer(\"russian\")\n",
        "stemmer_en = SnowballStemmer(\"english\")\n",
        "\n",
        "# Функция для лемматизации текста (русский язык)\n",
        "def lemmatize(text: str) -> list[str]:\n",
        "    words = re.findall(r'\\w+', text)  # Убираем пунктуацию\n",
        "    return [morph.parse(word)[0].normal_form for word in words]\n",
        "\n",
        "# Функция для стемминга текста (русский и английский)\n",
        "def stemming(text: str, language: str = 'ru') -> list[str]:\n",
        "    words = re.findall(r'\\w+', text)  # Разбиваем на слова\n",
        "    if language == 'ru':\n",
        "        return [stemmer_ru.stem(word) for word in words]\n",
        "    else:\n",
        "        return [stemmer_en.stem(word) for word in words]\n",
        "\n",
        "# Функция для удаления стоп-слов\n",
        "def remove_stop_words(text: str) -> list[str]:\n",
        "    words = re.findall(r'\\w+', text)  # Токенизация текста\n",
        "    return [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Функция для токенизации по буквам (каждый символ становится токеном)\n",
        "def tokenize_by_letters(text: str) -> list[str]:\n",
        "    return list(text.replace(\" \", \"\"))  # Разбиваем строку на символы и удаляем пробелы\n",
        "\n",
        "# Функция для векторизации текста\n",
        "def vectorize(tokens: list[str]) -> dict[int, str]:\n",
        "    return {index+1: token for index, token in enumerate(tokens)}\n",
        "\n",
        "# Пример использования\n",
        "text = \"Пример большого текста. Бегущий\"\n",
        "text_en = \"Let's test!\"\n",
        "\n",
        "# Лемматизация\n",
        "lemmatized_text = lemmatize(text)\n",
        "lemmatized_text_en = lemmatize(text_en)\n",
        "\n",
        "# Стемминг\n",
        "stemmed_text = stemming(text, language='ru')\n",
        "stemmed_text_en = stemming(text_en, language='en')\n",
        "\n",
        "# Удаление стоп-слов\n",
        "filtered_text = remove_stop_words(text)\n",
        "filtered_text_en = remove_stop_words(text_en)\n",
        "\n",
        "# Токенизация по буквам (по символам)\n",
        "tokenized_text_by_letters = tokenize_by_letters(text)\n",
        "tokenized_text_en_by_letters = tokenize_by_letters(text_en)\n",
        "\n",
        "# Векторизация\n",
        "vectorized_text = vectorize(tokenized_text_by_letters)\n",
        "vectorized_text_en = vectorize(tokenized_text_en_by_letters)\n",
        "\n",
        "# Результаты\n",
        "print(\"Результаты для русского текста:\")\n",
        "print(\"Лемматизация:\", lemmatized_text[:5])\n",
        "print(\"Стемминг:\", stemmed_text[:5])\n",
        "print(\"Удаление стоп-слов:\", filtered_text[:5])\n",
        "print(\"Токенизация по буквам (русский):\", tokenized_text_by_letters)\n",
        "print(\"Векторизация (русский):\", vectorized_text)\n",
        "\n",
        "print(\"\\nРезультаты для английского текста:\")\n",
        "print(\"Лемматизация:\", lemmatized_text_en[:5])\n",
        "print(\"Стемминг:\", stemmed_text_en[:5])\n",
        "print(\"Удаление стоп-слов:\", filtered_text_en[:5])\n",
        "print(\"Токенизация по буквам (английский):\", tokenized_text_en_by_letters)\n",
        "print(\"Векторизация (английский):\", vectorized_text_en)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lP7hA7w3MPR",
        "outputId": "5278f29b-6123-4c99-856c-48b2dd89ed37"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты для русского текста:\n",
            "Лемматизация: ['пример', 'большой', 'текст', 'бежать']\n",
            "Стемминг: ['пример', 'больш', 'текст', 'бегущ']\n",
            "Удаление стоп-слов: ['Пример', 'большого', 'текста', 'Бегущий']\n",
            "Токенизация по буквам (русский): ['П', 'р', 'и', 'м', 'е', 'р', 'б', 'о', 'л', 'ь', 'ш', 'о', 'г', 'о', 'т', 'е', 'к', 'с', 'т', 'а', '.', 'Б', 'е', 'г', 'у', 'щ', 'и', 'й']\n",
            "Векторизация (русский): {1: 'П', 2: 'р', 3: 'и', 4: 'м', 5: 'е', 6: 'р', 7: 'б', 8: 'о', 9: 'л', 10: 'ь', 11: 'ш', 12: 'о', 13: 'г', 14: 'о', 15: 'т', 16: 'е', 17: 'к', 18: 'с', 19: 'т', 20: 'а', 21: '.', 22: 'Б', 23: 'е', 24: 'г', 25: 'у', 26: 'щ', 27: 'и', 28: 'й'}\n",
            "\n",
            "Результаты для английского текста:\n",
            "Лемматизация: ['let', 's', 'test']\n",
            "Стемминг: ['let', 's', 'test']\n",
            "Удаление стоп-слов: ['Let', 's', 'test']\n",
            "Токенизация по буквам (английский): ['L', 'e', 't', \"'\", 's', 't', 'e', 's', 't', '!']\n",
            "Векторизация (английский): {1: 'L', 2: 'e', 3: 't', 4: \"'\", 5: 's', 6: 't', 7: 'e', 8: 's', 9: 't', 10: '!'}\n"
          ]
        }
      ]
    }
  ]
}